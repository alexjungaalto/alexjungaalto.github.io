<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Alexander Jung</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">menu</div>
<div class="menu-item"><a href="index.html" class="current">home</a></div>
<div class="menu-item"><a href="courses.html">courses</a></div>
<div class="menu-item"><a href="publications.html">publications</a></div>
<div class="menu-item"><a href="supervision.html">supervision</a></div>
<div class="menu-item"><a href="projects.html">projects</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Alexander Jung</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://github.com/alexjungaalto/MachineLearningTheBasics/blob/master/MLBasicsBook.pdf"><img src="MLBook.png" alt="alt text" width="200px" /></a>&nbsp;</td>
<td align="left"><ul>
<li><p>Dipl.-Ing. Dr. techn. (<a href="https://en.wikipedia.org/wiki/Sub_auspiciis_Praesidentis"  target="_blank">"sub auspiciis"</a>) </p>
</li>
<li><p>Assistant Professor for Machine Learning, Aalto University<br /> </p>
</li>
<li><p>Associate Editor for IEEE Signal Processing Letters (<a href="https://signalprocessingsociety.org/publications-resources/ieee-signal-processing-letters/editorial-board"  target="_blank">website</a>) <br /></p>
</li>
<li><p>Chapter Chair IEEE Finland Jt. Chapter SP-CAS (<a href="https://r8.ieee.org/finland-spcas/"  target="_blank">website</a>)   <br /> </p>
</li>
<li><p>Board Member Finnish Union of University Researchers and Teachers (FUURT) (<a href="https://tieteentekijat.fi/en/union-close-to-you/union-board/"  target="_blank">website</a>) <br /></p>
</li>
<li><p>Textbook: &ldquo;Machine Learning - The Basics&rdquo;, Springer, 2022 (<a href="http://mlbook.cs.aalto.fi"  target="_blank">"draft"</a> ) <br /></p>
</li>
</ul>
</td></tr></table>
<h2>About me</h2>
<p>I have received a Dipl.-Ing. (MSc) and Dr.techn. (Phd) degree in electrical engineering and signal processing from TU Vienna in 2008 and 2012, respectively. Since 2015, I am an Assistant 
Professor for Machine Learning at the Department of Computer Science of Aalto University. I am leading the research group <b>Machine Learning for Big Data</b> which is researching and teaching 
the mathematical foundations of machine learning. </p>
<h2>NEWS </h2>
<ul>
<li><p>We will organize an <a href="https://signalprocessingsociety.org/community-involvement/seasonal-schools" target="_blank"> IEEE SPS Seasonal School </a> on Networked Federated Learning, <a href="https://ieeespcasfinland.github.io/"  target="_blank">School Site</a></p>
</li>
<li><p><a href="https://www.isit2022.org/authors/call-for-tutorials/"  target="_blank">Call for Tutorials</a> for the 2022 IEEE Int. Symp. on Inf. Th. (ISIT) held during June and July 2022 at Aalto University, Finland </p>
</li>
</ul>
<h2>Research Highlight: Federated Multitask Learning from Big Data over Networks </h2>
<p><img src="Pandemics.jpg" width="500" align="left" /> Many important application domains generate collections of local datasets that are related by an intrinsic network structure (&ldquo;big data over networks&rdquo;). 
A timely application domain that generates such big data over networks is the management of pandemics. Individuals generate local datasets via their smartphones and wearables that measure biophysical 
parameters. The statistical properties local datasets are related via different network structures that reflect physical (&ldquo;contact networks&rdquo;), social or biological proximity. While local datasets typically do not 
conform to the i.i.d. assumption, we can approximate local datasets forming a tight-knit cluster as i.i.d. realizations. 
<br /> </p>
<p>To capitalize on the information in local datasets and their network structure, we have recently proposed networked exponential families as a novel probabilistic model for big data over networks. 
Networked exponential families are appealing statistically and computationally. They allow to adaptively pool local datasets with similar statistical properties as training sets to learn personalized 
predictions tailored to each local dataset. We can compute these personalized predictions using highly scalable distributed convex optimization methods. 
This methods are robust against various types of failures and do not require the exchange of (potentially sensitive) raw data.</p>
<p><b>Relevant Publications:</b></p>
<ul>
<li><p>A. Jung, &ldquo;Federated Learning Over Networks for Pandemics&rdquo;, LiveProject at Manning.com, 2021, <a href="https://www.manning.com/liveprojectseries/federated-learning-ser"  target="_blank"> website </a> </p>
</li>
<li><p>Y. Sarcheshmehpour, M Leinonen and A. Jung, “Federated Learning From Big Data Over Networks”, at IEEE ICASSP, 2021. preprint: https:<i></i>arxiv.org<i>abs</i>2010.14159</p>
</li>
<li><p>A. Jung, &ldquo;Networked Exponential Families for Big Data Over Networks,&rdquo; in IEEE Access, vol. 8, pp. 202897-202909, 2020, doi: 10.1109/ACCESS.2020.3033817.</p>
</li>
<li><p>A. Jung and N. Tran, &ldquo;Localized Linear Regression in Networked Data,&rdquo; in IEEE Signal Processing Letters, vol. 26, no. 7, pp. 1090-1094, July 2019, doi: 10.1109/LSP.2019.2918933.</p>
</li>
<li><p>N. Tran, O. Abramenko and A. Jung, &ldquo;On the Sample Complexity of Graphical Model Selection From Non-Stationary Samples,&rdquo; in IEEE Transactions on Signal Processing, vol. 68, pp. 17-32, 2020, doi: 10.1109/TSP.2019.2956687.</p>
</li>
</ul>
<h2>Research Highlight: Personalized Explainable Machine Learning </h2>
<p><img src="ProbModelXML.png" width="500" align="left" /> A key challenge for the widespread use of machine learning methods is the explainability of their predictions. 
We have recently developed a novel approach to constructing personalized explanations for the predictions delivered by machine learning method. We measure the effect 
of an explanation by the reduction in the conditional entropy of the prediction given the summary that a particular user associates with data points. The user summary is 
used to characterise the background knowledge of the &ldquo;explainee&rdquo; in order to compute explanations that are tailored for her. To compute the explanations our method only
requires some training samples that consists of data points and their corresponding predictions and user summaries. Thus, our method is model agnostic and can be used 
to compute explanations for different machine learning methods. </p>
<p><b>Relevant Publications:</b></p>
<ul>
<li><p>A. Jung, “Explainable Empirical Risk Minimization”, arXiv eprint, 2020. <a href="https://arxiv.org/abs/2009.01492">weblink</a></p>
</li>
<li><p>A. Jung and P. H. J. Nardelli, &ldquo;An Information-Theoretic Approach to Personalized Explainable Machine Learning,&rdquo; in IEEE Signal Processing Letters, vol. 27, pp. 825-829, 2020, doi: 10.1109/LSP.2020.2993176.</p>
</li>
</ul>
<h2>Teaching Highlight: Student Feedback-Driven Course Development</h2>
<p><img src="ThreeCompCycle.png" width="500" align="left" /> Right from my start at Aalto in 2015, I took care of the main machine learning courses at Aalto University. 
Within three years I have re-designed the spearhead course Machine Learning: Basic Principles (MLBP). This re-design was based on a careful analysis of feedback received from 
several thousands of students. I have also started to prepare <a href="mlcourseresponse.pdf"  target="_blank">response letters</a> to the student feedback, 
as it is customary in the review process of scientific journals. My final edition of MLBP in 2018 has achieved the best student rating since the course was established 
at Aalto. The efforts have also been acknowledged by the <a href="TeacherAward.png"  target="_blank">Teacher of the Year</a> award, which I have received in 2018 from the Department of Computer Science at Aalto University.</p>
<h2>Teaching Highlight: A Three-Component Picture of Machine Learning</h2>
<p>Machine learning methods have been and are currently popularized in virtually any field of science and technology. As a result, machine learning courses attract students from different study programs. Thus, a key challenge in teaching basic machine learning courses is the heterogeneity of student backgrounds. To cope with this challenge, I have developed a new teaching concept for machine learning. This teaching concept revolves around three main components of machine learning: data, models and loss functions. By decomposing every machine learning methods into 
specific design choices for data representation, model and loss function, students learn to navigate the vast landscape of machine learning methods and applications. The three-component picture of machine learning is the main subject of my textbook <a href="https://github.com/alexjungaalto/MachineLearningTheBasics/blob/master/MLBasicsBook.pdf" target="_blank">Machine Learning: The Basics</a>. </p>
<p><p><img src="MLLandscape.png" width="500" align="bottom" />  </p></p>
<div id="footer">
<div id="footer-text">
Page generated 2022-02-07 17:43:16 CET, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
